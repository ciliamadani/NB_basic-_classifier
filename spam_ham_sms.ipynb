{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re \n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHODES\n",
    "\n",
    "\"\"\" Remove stop words\"\"\"\n",
    "def remove_stop_words(text):\n",
    "    for w in text:\n",
    "        if w in stop_words:\n",
    "                text= re.sub(w,\" \", text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "\"\"\" Join all he text belonging to a particuler class \"\"\"\n",
    "def join_msgs (msgs):\n",
    "    return \" \".join(m[1].lower() for m in msgs)\n",
    "\n",
    "\"\"\" Split msgs into words based on the whitespace and then count the occurence of each word \"\"\"\n",
    "\n",
    "def split_msgs(msgs):\n",
    "    words = re.split('\\s+', msgs)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Calcuate the probabilty of a msg belong to a specific class \"\"\"\n",
    "\n",
    "def predict_class(msg, class_proba, class_count,class_cnt):\n",
    "    # Calculate the frequency of words in both ham and spam classes\n",
    "    prediction = 1\n",
    "    \n",
    "    # prediction *= (words_count.get(w)*(class_count.get(w,0)+1)) / (sum(class_count.values())+class_cnt)\n",
    "    \n",
    "    words_count = split_msgs(msg)\n",
    "    for w in words_count:\n",
    "        prediction *= (words_count.get(w)*(class_count.get(w,0)+1)) / (sum(class_count.values())+class_cnt)\n",
    "        \n",
    "    return (prediction * class_proba )\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-420-fbed10951ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#Remove anything that is not a letter or a whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-z\\s]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#Replace excessive spacing by just one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('/home/cilia/Téléchargements/ML/sms_spam_collection_dataset/spam.csv', encoding='latin-1') as file:\n",
    "    msgs =list(csv.reader(file)) \n",
    "    \n",
    "#Generate the english stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#SOME DATA CLEANING +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "for s in msgs:\n",
    "    for i in range(3):\n",
    "        s.pop()\n",
    "        #Remove anything that is not a letter or a whitespace\n",
    "        s[1]= re.sub(\"[^a-z\\s]+\", \" \", s[1], flags=re.IGNORECASE)\n",
    "        \n",
    "        #Replace excessive spacing by just one\n",
    "        s[1]= re.sub(\"(\\s+)\", \" \", s[1])\n",
    "        s[1]= remove_stop_words(s[1])\n",
    "      \n",
    "\n",
    " #Del the name of columns         \n",
    "del(msgs[0])\n",
    "\n",
    "\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = [s for s in msgs if s[0]== 'spam']\n",
    "ham_sms  = [s for s in msgs if s[0]== 'ham']\n",
    "\n",
    "spam_text= join_msgs(spam_sms)\n",
    "ham_text = join_msgs(ham_sms)\n",
    "\n",
    "spam_count = split_msgs(spam_text)\n",
    "ham_count = split_msgs(ham_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the class probabilities \n",
    "prob_spam = len(spam_sms) / len(data['sms'])\n",
    "prob_ham  = len(ham_sms)  / len(data['sms'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "msg = \"\"\n",
    "msg = remove_stop_words(msg)\n",
    "\n",
    "s_predic = predict_class(msg, prob_spam,spam_words, len(spam_words))\n",
    "h_predic = predict_class(msg, prob_ham,ham_words, len(ham_words))\n",
    "\n",
    "if(s_predic > h_predic):\n",
    "    print('spam')\n",
    "else:\n",
    "    print('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
